{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AgeGender.ipynb","provenance":[{"file_id":"https://github.com/droidadroit/age-and-gender-classification/blob/master/AgeGender.ipynb","timestamp":1624615634622}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"7QQe6I3yhCY_"},"source":["# Introduction\n","This is the implementation of [Age and Gender Classification by Gil Levi and Tal Hassner](https://talhassner.github.io/home/projects/cnn_agegender/CVPR2015_CNN_AgeGenderEstimation.pdf) using PyTorch as the deep learning framework and Google Colab as the training ground. The network proposed in the paper has five convolutional layers and three fully connected layers, and is simple enough to understand and get familiar with PyTorch and Colab. Try it!\n","\n","Instead, if you just want to test the model on an image, go to the **Testing on an image** section."]},{"cell_type":"markdown","metadata":{"id":"XCz5QH4dlh_H"},"source":["# Setup"]},{"cell_type":"markdown","metadata":{"id":"arTKN7rkgZsR"},"source":["Ensure that you are using Python 3.6 and GPU as the hardware accelerator. To check this, go to **Runtime -> Change runtime type** in the menu bar of Colab."]},{"cell_type":"markdown","metadata":{"id":"O5NLRAhllX29"},"source":["## Mounting the drive"]},{"cell_type":"code","metadata":{"id":"uRVNRKKSVNLK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624729799879,"user_tz":-360,"elapsed":2717297,"user":{"displayName":"Khondoker Ittehadul Islam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GggB1hRCScVHSb8B6_geMRo9IveQWMtqATJkX1z=s64","userId":"10124878052441446029"}},"outputId":"e0d73889-6218-44be-c9be-1848a9dfe762"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Kpr2iINzADlV"},"source":["## Directory structure\n","Our project is in the directory **AgeGenderClassification**. Create this directory and the sub-directories inside it along with the downloaded **AgeGender.ipynb**.\n","\n","content  \n","-------- gdrive  \n","---------------- My Drive  \n","------------------------ **AgeGenderClassification**  \n","-------------------------------- **data**  \n","-------------------------------- **models**  \n","-------------------------------- **results**  \n","-------------------------------- **AgeGender.ipynb**\n"]},{"cell_type":"markdown","metadata":{"id":"wQvenRKomUkx"},"source":["## Installing PyTorch\n","This script installs PyTorch and torchvision."]},{"cell_type":"code","metadata":{"id":"MRHSqYujmgn-","colab":{"base_uri":"https://localhost:8080/","height":384},"executionInfo":{"status":"error","timestamp":1624729800678,"user_tz":-360,"elapsed":806,"user":{"displayName":"Khondoker Ittehadul Islam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GggB1hRCScVHSb8B6_geMRo9IveQWMtqATJkX1z=s64","userId":"10124878052441446029"}},"outputId":"32a96960-b4d2-4b09-e748-f91982943a65"},"source":["from os.path import exists\n","from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n","platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n","cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n","accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n","\n","!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision"],"execution_count":2,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-3cfac2b01444>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mwheel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpep425tags\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_abbr_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_impl_ver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_abi_tag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplatform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'{}{}-{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_abbr_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_impl_ver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_abi_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcuda_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetoutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ldconfig -p|grep cudart.so|sed -e 's/.*\\\\.\\\\([0-9]*\\\\)\\\\.\\\\([0-9]*\\\\)$/cu\\\\1\\\\2/'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0maccelerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcuda_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/dev/nvidia0'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wheel.pep425tags'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]},{"cell_type":"markdown","metadata":{"id":"UkNgiJoSm87s"},"source":["## Imports"]},{"cell_type":"code","metadata":{"id":"BRaqXIPonB8j","executionInfo":{"status":"aborted","timestamp":1624729800683,"user_tz":-360,"elapsed":11,"user":{"displayName":"Khondoker Ittehadul Islam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GggB1hRCScVHSb8B6_geMRo9IveQWMtqATJkX1z=s64","userId":"10124878052441446029"}}},"source":["import torch\n","import torch.autograd.variable as Variable\n","import torchvision\n","import torchvision.transforms as transforms\n","import torch.optim as optim\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision.utils as utils\n","from torch.utils.data import Dataset, DataLoader"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1wIv9zyxtHxt","executionInfo":{"status":"aborted","timestamp":1624729800684,"user_tz":-360,"elapsed":12,"user":{"displayName":"Khondoker Ittehadul Islam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GggB1hRCScVHSb8B6_geMRo9IveQWMtqATJkX1z=s64","userId":"10124878052441446029"}}},"source":["import numpy as np\n","import os\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","from shutil import copyfile"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0_z6Q972xi32"},"source":["# Preparing dataloaders"]},{"cell_type":"markdown","metadata":{"id":"s_KtxziRPrVI"},"source":["## **Raw data**\n","\n"]},{"cell_type":"markdown","metadata":{"id":"8csd2D2v4-0O"},"source":["### Downloading data\n","\n","\n","We use the Adience dataset consisting unfiltered faces ([Link](http://www.cslab.openu.ac.il/download/adiencedb/AdienceBenchmarkOfUnfilteredFacesForGenderAndAgeClassification/aligned.tar.gz)). Then, we unzip it.  \n","The first cell below downloads the data for you and places it in the **data** directory. The second cell unzips the data."]},{"cell_type":"code","metadata":{"id":"4Tqb-HEKTLLo","executionInfo":{"status":"aborted","timestamp":1624729800684,"user_tz":-360,"elapsed":12,"user":{"displayName":"Khondoker Ittehadul Islam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GggB1hRCScVHSb8B6_geMRo9IveQWMtqATJkX1z=s64","userId":"10124878052441446029"}}},"source":["!wget --user adiencedb --password adience http://www.cslab.openu.ac.il/download/adiencedb/AdienceBenchmarkOfUnfilteredFacesForGenderAndAgeClassification/aligned.tar.gz -P \"/content/gdrive/My Drive/AgeGenderClassification/data\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NPeBSiNqimxw","executionInfo":{"status":"aborted","timestamp":1624729800685,"user_tz":-360,"elapsed":13,"user":{"displayName":"Khondoker Ittehadul Islam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GggB1hRCScVHSb8B6_geMRo9IveQWMtqATJkX1z=s64","userId":"10124878052441446029"}}},"source":["!tar xvzf \"/content/gdrive/My Drive/AgeGenderClassification/data/aligned.tar.gz\" -C \"/content/gdrive/My Drive/AgeGenderClassification/data/\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"88F5gWj5o4bh"},"source":["### Downloading folds\n","\n","All five folds used in this paper are present [here](https://github.com/GilLevi/AgeGenderDeepLearning/tree/master/Folds/train_val_txt_files_per_fold). Download the **train_val_txt_files_per_fold** folder and place it in **My Drive/AgeGenderClassification/data**.\n"]},{"cell_type":"markdown","metadata":{"id":"2VtoAJghnpLn"},"source":["## Data loading"]},{"cell_type":"code","metadata":{"id":"squb12xguXqd","executionInfo":{"status":"aborted","timestamp":1624729800685,"user_tz":-360,"elapsed":13,"user":{"displayName":"Khondoker Ittehadul Islam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GggB1hRCScVHSb8B6_geMRo9IveQWMtqATJkX1z=s64","userId":"10124878052441446029"}}},"source":["PATH_TO_FOLDS = \"/content/gdrive/My Drive/AgeGenderClassification/data/train_val_txt_files_per_fold\"\n","PATH_TO_DATA = \"/content/gdrive/My Drive/AgeGenderClassification/data\"\n","PATH_TO_IMAGE_FOLDERS = PATH_TO_DATA + \"/aligned\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O2Czi4va4QKo"},"source":["### Creating a Dataset class\n","\n","We create a class **`AdienceDataset`** that extends **`Dataset`**. This class helps us in feeding the input data to the network in minibatches.\n","\n","[This](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html) is a useful tutorial on how to load and augment data in PyTorch. "]},{"cell_type":"code","metadata":{"id":"BiUufbSinr1K","executionInfo":{"status":"aborted","timestamp":1624729800686,"user_tz":-360,"elapsed":14,"user":{"displayName":"Khondoker Ittehadul Islam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GggB1hRCScVHSb8B6_geMRo9IveQWMtqATJkX1z=s64","userId":"10124878052441446029"}}},"source":["class AdienceDataset(Dataset):\n","    \n","    def __init__(self, txt_file, root_dir, transform):\n","        self.txt_file = txt_file\n","        self.root_dir = root_dir\n","        self.transform = transform\n","        self.data = self.read_from_txt_file()\n","    \n","    def __len__(self):\n","        return len(self.data)\n","\n","    def read_from_txt_file(self):\n","        data = []\n","        f = open(self.txt_file)\n","        for line in f.readlines():\n","            image_file, label = line.split()\n","            label = int(label)\n","            if 'gender' in self.txt_file:\n","                label += 8\n","            data.append((image_file, label))\n","        return data\n","    \n","    def __getitem__(self, idx):\n","        img_name, label = self.data[idx]\n","        image = Image.open(self.root_dir + '/' + img_name)\n","        \n","        if self.transform:\n","            image = self.transform(image)\n","            \n","        return {\n","            'image': image,\n","            'label': label\n","        }         "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WAx-8hZNsXFM"},"source":["### Transforms\n","Every image is first resized to a `256x256` image and then cropped to a `227x227` image before being fed to the network.\n","\n","**`transforms_list`** is the list of transforms we would like to apply to the input data. Apart from training the neural network without any transformations, we can also train the network using the following transforms (also called as data augmentation techniques):\n","*   random horizontal flip\n","*   random crop and random horizontal flip\n","\n","We don't perform any transformation on the images during validation and testing.\n"]},{"cell_type":"code","metadata":{"id":"_wMCsfG1sY4s","executionInfo":{"status":"aborted","timestamp":1624729800686,"user_tz":-360,"elapsed":14,"user":{"displayName":"Khondoker Ittehadul Islam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GggB1hRCScVHSb8B6_geMRo9IveQWMtqATJkX1z=s64","userId":"10124878052441446029"}}},"source":["transforms_list = [\n","    transforms.Resize(256),\n","    transforms.CenterCrop(227),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.RandomCrop(227)\n","]\n","\n","transforms_dict = {\n","    'train': {\n","        0: list(transforms_list[i] for i in [0, 1, 3]),        # no transformation\n","        1: list(transforms_list[i] for i in [0, 1, 2, 3]),     # random horizontal flip\n","        2: list(transforms_list[i] for i in [0, 4, 2, 3])      # random crop and random horizontal flip\n","    },\n","    'val': {\n","        0: list(transforms_list[i] for i in [0, 1, 3])\n","    },\n","    'test': {\n","        0: list(transforms_list[i] for i in [0, 1, 3])\n","    }\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iURni95m4sGW"},"source":["### Dataloader\n","The **`DataLoader`** class in PyTorch helps us iterate through the dataset. This is where we input **`minibatch_size`** to our algorithm."]},{"cell_type":"code","metadata":{"id":"ox_PryLS4u2x","executionInfo":{"status":"aborted","timestamp":1624729800687,"user_tz":-360,"elapsed":14,"user":{"displayName":"Khondoker Ittehadul Islam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GggB1hRCScVHSb8B6_geMRo9IveQWMtqATJkX1z=s64","userId":"10124878052441446029"}}},"source":["def get_dataloader(s, c, fold, transform_index, minibatch_size):\n","    \"\"\"\n","    Args:\n","        s: A string. Equals either \"train\", \"val\", or \"test\".\n","        c: A string. Equals either \"age\" or \"gender\".\n","        fold: An integer. Lies in the range [0, 4] as there are five folds present.\n","        transform_index: An integer. The transforms in the list correesponding\n","            to this index in the dictionary will be applied on the images.\n","        minibatch_size: An integer.\n","\n","    Returns:\n","        An instance of the DataLoader class.\n","    \"\"\"\n","    txt_file = f'{PATH_TO_FOLDS}/test_fold_is_{fold}/{c}_{s}.txt'\n","    root_dir = PATH_TO_IMAGE_FOLDERS\n","    \n","    transformed_dataset = AdienceDataset(txt_file, root_dir,\n","                                         transforms.Compose(transforms_dict[s][transform_index]))\n","    dataloader = DataLoader(transformed_dataset, batch_size=minibatch_size, shuffle=True, num_workers=4)\n","    \n","    return dataloader"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9lrstEuSm1qH"},"source":["# Network"]},{"cell_type":"code","metadata":{"id":"o9feSnLcFq2q","executionInfo":{"status":"aborted","timestamp":1624729800687,"user_tz":-360,"elapsed":14,"user":{"displayName":"Khondoker Ittehadul Islam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GggB1hRCScVHSb8B6_geMRo9IveQWMtqATJkX1z=s64","userId":"10124878052441446029"}}},"source":["device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EJ2QrXRPmCGs","executionInfo":{"status":"aborted","timestamp":1624729800687,"user_tz":-360,"elapsed":14,"user":{"displayName":"Khondoker Ittehadul Islam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GggB1hRCScVHSb8B6_geMRo9IveQWMtqATJkX1z=s64","userId":"10124878052441446029"}}},"source":["PATH_TO_MODELS = \"/content/gdrive/MyDrive/AgeGenderClassification/models\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SZh1psZ6mY6d"},"source":["## Defining the network\n","This is the network as described in the [paper](https://talhassner.github.io/home/projects/cnn_agegender/CVPR2015_CNN_AgeGenderEstimation.pdf)."]},{"cell_type":"code","metadata":{"id":"pd91hyHvmhYP","executionInfo":{"status":"aborted","timestamp":1624729800688,"user_tz":-360,"elapsed":15,"user":{"displayName":"Khondoker Ittehadul Islam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GggB1hRCScVHSb8B6_geMRo9IveQWMtqATJkX1z=s64","userId":"10124878052441446029"}}},"source":["class Net(nn.Module):\n","    \n","    def __init__(self):\n","        super(Net, self).__init__()\n","        \n","        self.conv1 = nn.Conv2d(3, 96, 7, stride = 4, padding = 1)\n","        self.pool1 = nn.MaxPool2d(3, stride = 2, padding = 1)\n","        self.norm1 = nn.LocalResponseNorm(size = 5, alpha = 0.0001, beta = 0.75)\n","        \n","        self.conv2 = nn.Conv2d(96, 256, 5, stride = 1, padding = 2)\n","        self.pool2 = nn.MaxPool2d(3, stride = 2, padding = 1)\n","        self.norm2 = nn.LocalResponseNorm(size = 5, alpha = 0.0001, beta = 0.75)\n","        \n","        self.conv3 = nn.Conv2d(256, 384, 3, stride = 1, padding = 1)\n","        self.pool3 = nn.MaxPool2d(3, stride = 2, padding = 1)\n","        self.norm3 = nn.LocalResponseNorm(size = 5, alpha = 0.0001, beta = 0.75)\n","        \n","        self.fc1 = nn.Linear(18816, 512)\n","        self.dropout1 = nn.Dropout(0.5)\n","\n","        self.fc2 = nn.Linear(512, 512)\n","        self.dropout2 = nn.Dropout(0.5)\n","  \n","        self.fc3 = nn.Linear(512, 10)\n","    \n","        self.apply(weights_init)\n","\n","    \n","    def forward(self, x):\n","        x = F.leaky_relu(self.conv1(x))\n","        x = self.pool1(x)\n","        x = self.norm1(x)\n","\n","        x = F.leaky_relu(self.conv2(x))\n","        x = self.pool2(x)\n","        x = self.norm2(x)\n","      \n","        x = F.leaky_relu(self.conv3(x))\n","        x = self.pool3(x)\n","        x = self.norm3(x)\n","      \n","        x = x.view(-1, 18816)\n","        \n","        x = self.fc1(x)\n","        x = F.leaky_relu(x)\n","        x = self.dropout1(x)\n","      \n","        x = self.fc2(x)\n","        x = F.leaky_relu(x)\n","        x = self.dropout2(x)\n","      \n","        x = F.log_softmax(self.fc3(x), dim=1)\n","  \n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uaI0Bs-Rxio-","executionInfo":{"status":"aborted","timestamp":1624729800688,"user_tz":-360,"elapsed":15,"user":{"displayName":"Khondoker Ittehadul Islam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GggB1hRCScVHSb8B6_geMRo9IveQWMtqATJkX1z=s64","userId":"10124878052441446029"}}},"source":["def weights_init(m):\n","    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n","        nn.init.normal_(m.weight, mean=0, std=1e-2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FrLA7cfxj4JE","executionInfo":{"status":"aborted","timestamp":1624729800689,"user_tz":-360,"elapsed":16,"user":{"displayName":"Khondoker Ittehadul Islam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GggB1hRCScVHSb8B6_geMRo9IveQWMtqATJkX1z=s64","userId":"10124878052441446029"}}},"source":["criterion = nn.NLLLoss()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qCTDc4XEyqOq"},"source":["## Hyperparameters\n","Try playing with these! While the **`minibatch_size`** and **`lr`** are pulled from the paper, **`num_epochs`** is set empirically. "]},{"cell_type":"code","metadata":{"id":"w5Vm2Tb2yzYT","executionInfo":{"status":"aborted","timestamp":1624729800689,"user_tz":-360,"elapsed":16,"user":{"displayName":"Khondoker Ittehadul Islam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GggB1hRCScVHSb8B6_geMRo9IveQWMtqATJkX1z=s64","userId":"10124878052441446029"}}},"source":["minibatch_size = 50\n","num_epochs = 60\n","lr = 0.0001  # initial learning rate"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G5NPiENkmpYl"},"source":["## Training the network\n","We save the network to the drive and compute the loss on validation set after every **`checkpoint_frequency`** number of iterations. We decrease the learning by a tenth after 10,000 iterations using the **`MultiStepLR`** class of PyTorch."]},{"cell_type":"code","metadata":{"id":"NnkM7TgymrED","executionInfo":{"status":"aborted","timestamp":1624729800690,"user_tz":-360,"elapsed":16,"user":{"displayName":"Khondoker Ittehadul Islam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GggB1hRCScVHSb8B6_geMRo9IveQWMtqATJkX1z=s64","userId":"10124878052441446029"}}},"source":["def train(net, train_dataloader, epochs, filename, checkpoint_frequency, val_dataloader=None):\n","    \"\"\"\n","    Args:\n","        net: An instance of PyTorch's Net class.\n","        train_dataloader: An instance of PyTorch's Dataloader class.\n","        epochs: An integer.\n","        filename: A string. Name of the model saved to drive.\n","        checkpoint_frequency: An integer. Represents how frequent (in terms\n","            of number of iterations) the model should be saved to drive.\n","        val_dataloader: An instance of PyTorch's Dataloader class.\n","    \n","    Returns:\n","        net: An instance of PyTorch's Net class. The trained network.\n","        training_loss: A list of numbers that represents the training loss at each checkpoint.\n","        validation_loss: A list of numbers that represents the validation loss at each checkpoint.\n","    \"\"\"\n","    net.train()\n","    optimizer = optim.Adam(net.parameters(), lr)\n","    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10000])\n","    \n","    training_loss, validation_loss = [], []\n","    checkpoint = 0\n","    iteration = 0\n","    running_loss = 0\n","    \n","    for epoch in range(epochs):\n","        \n","        for i, batch in enumerate(train_dataloader):\n","            scheduler.step()\n","            optimizer.zero_grad()\n","            images, labels = batch['image'].to(device), batch['label'].to(device)\n","            outputs = net(images)\n","            loss = criterion(outputs, labels)\n","            running_loss += float(loss.item())\n","            loss.backward()\n","            optimizer.step()\n","                                    \n","            if (iteration+1) % checkpoint_frequency == 0 and val_dataloader is not None:\n","                training_loss.append(running_loss/checkpoint_frequency)\n","                validation_loss.append(validate(net, val_dataloader))\n","                print(f'minibatch:{i}, epoch:{epoch}, iteration:{iteration}, training_error:{training_loss[-1]}, validation_error:{validation_loss[-1]}')\n","                save_network(net, f'{filename}_checkpoint{checkpoint}')\n","                checkpoint += 1\n","                running_loss = 0\n","            \n","            iteration += 1\n","\n","    return net, training_loss, validation_loss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x7WndciFmv0C"},"source":["## Validation\n","We evaluate the performance (in terms of loss) of the trained network on validation set."]},{"cell_type":"code","metadata":{"id":"8Es1uGKQnK9D","executionInfo":{"status":"aborted","timestamp":1624729800691,"user_tz":-360,"elapsed":17,"user":{"displayName":"Khondoker Ittehadul Islam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GggB1hRCScVHSb8B6_geMRo9IveQWMtqATJkX1z=s64","userId":"10124878052441446029"}}},"source":["def validate(net, dataloader):\n","    net.train()\n","    total_loss = 0\n","    with torch.no_grad():\n","        for i, batch in enumerate(dataloader):\n","            images, labels = batch['image'].to(device), batch['label'].to(device)\n","            outputs = net(images)\n","            loss = criterion(outputs, labels)\n","            total_loss += float(loss.item())\n","\n","    return total_loss/(i+1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sUyMnXqwauw9","executionInfo":{"status":"aborted","timestamp":1624729800692,"user_tz":-360,"elapsed":18,"user":{"displayName":"Khondoker Ittehadul Islam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GggB1hRCScVHSb8B6_geMRo9IveQWMtqATJkX1z=s64","userId":"10124878052441446029"}}},"source":["def get_validation_error(c, fold, train_transform_index):\n","    filename = get_model_filename(c, fold, train_transform_index)\n","    net = Net().to(device)\n","    net.load_state_dict(torch.load(f'{PATH_TO_MODELS}/{filename}'))\n","    return validate(net, get_dataloader('val', c, fold, 0, minibatch_size))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gAto0iO4zPLw"},"source":["## Testing\n","We evaluate the performance (in terms of accuracy) of the trained network on the test set."]},{"cell_type":"code","metadata":{"id":"fCUCNrL0zR3n","executionInfo":{"status":"aborted","timestamp":1624729800692,"user_tz":-360,"elapsed":18,"user":{"displayName":"Khondoker Ittehadul Islam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GggB1hRCScVHSb8B6_geMRo9IveQWMtqATJkX1z=s64","userId":"10124878052441446029"}}},"source":["def test(net, dataloader, c):\n","    result = {\n","        'exact_match': 0,\n","        'total': 0\n","    }\n","    if c == 'age':\n","        result['one_off_match'] = 0\n","\n","    with torch.no_grad():\n","        net.eval()\n","        for i, batch in enumerate(dataloader):\n","            images, labels = batch['image'].to(device), batch['label'].to(device)\n","            outputs = net(images)\n","            outputs = torch.tensor(list(map(lambda x: torch.max(x, 0)[1], outputs))).to(device)\n","            result['total'] += len(outputs)\n","            result['exact_match'] += sum(outputs == labels).item()\n","            if c == 'age':\n","                result['one_off_match'] += (sum(outputs==labels) +\n","                                            sum(outputs==labels-1) +\n","                                            sum(outputs==labels+1)).item()\n","\n","    return result           "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mSq61rZ-yOPZ"},"source":["## Saving the network"]},{"cell_type":"code","metadata":{"id":"YzgCR7VqyQU6","executionInfo":{"status":"aborted","timestamp":1624729800693,"user_tz":-360,"elapsed":19,"user":{"displayName":"Khondoker Ittehadul Islam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GggB1hRCScVHSb8B6_geMRo9IveQWMtqATJkX1z=s64","userId":"10124878052441446029"}}},"source":["def save_network(net, filename):\n","    torch.save(net.state_dict(), f'{PATH_TO_MODELS}/{filename}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hl1owOOw0ZeU"},"source":["# Execution"]},{"cell_type":"markdown","metadata":{"id":"fpye5rRQv2e6"},"source":["### Picking the best model for a fold\n","**`train_save()`** trains the network using the **`train()`** function and then, using the validation losses returned by this function at all checkpoints, chooses the model with least validation error. This function also plots a graph of training and validation errors over the iterations.\n","\n","**Usage:**\n","\n","For e.g., if you want to train the network for **`age`** using **`fold=2`** and **`train_transform_index=2`**,\n","```\n","train_save('age', 2, 2)\n","```\n","\n"]},{"cell_type":"code","metadata":{"id":"ravxG5ce0bz0","executionInfo":{"status":"aborted","timestamp":1624729800694,"user_tz":-360,"elapsed":20,"user":{"displayName":"Khondoker Ittehadul Islam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GggB1hRCScVHSb8B6_geMRo9IveQWMtqATJkX1z=s64","userId":"10124878052441446029"}}},"source":["def train_save(c, fold, train_transform_index, checkpoint_frequency=50):\n","    \"\"\"\n","    Args:\n","        c: A string. Equals either \"age\" or \"gender\".\n","        fold: An integer. Lies in the range [0, 4] as there are five folds present.\n","        train_transform_index: An integer. The transforms in the list correesponding\n","            to this index in the dictionary will be applied on the images.\n","        checkpoint_frequency: An integer. Represents how frequent (in terms\n","            of number of iterations) the model should be saved to drive.   \n","    Returns:\n","        validation_loss: A list of numbers that represents the validation loss at each checkpoint.\n","    \"\"\"\n","    trained_net, training_loss, validation_loss = train(\n","        Net().to(device),\n","        get_dataloader('train', c, fold, train_transform_index, minibatch_size),\n","        num_epochs,\n","        f'{fold}_{c}_train_{train_transform_index}',\n","        checkpoint_frequency,\n","        get_dataloader('val', c, fold, 0, minibatch_size)\n","    )\n","    \n","    plt.plot(list(map(lambda x: checkpoint_frequency * x, (list(range(1, len(validation_loss)+1))))), validation_loss, label='validation_loss')\n","    plt.plot(list(map(lambda x: checkpoint_frequency * x, (list(range(1, len(training_loss)+1))))), training_loss, label='training_loss')\n","    plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3,\n","           ncol=2, mode=\"expand\", borderaxespad=0.)\n","    plt.xlabel('iterations')\n","    plt.ylabel('loss')\n","    plt.show()\n","    \n","    choose_model_with_least_val_error(c, fold, train_transform_index, validation_loss)\n","    \n","    return validation_loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1bYLxmb_JupM","executionInfo":{"status":"aborted","timestamp":1624729800695,"user_tz":-360,"elapsed":21,"user":{"displayName":"Khondoker Ittehadul Islam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GggB1hRCScVHSb8B6_geMRo9IveQWMtqATJkX1z=s64","userId":"10124878052441446029"}}},"source":["def choose_model_with_least_val_error(c, fold, train_transform_index, validation_loss):\n","    index = validation_loss.index(min(validation_loss))\n","    filename = f'{fold}_{c}_train_{train_transform_index}'\n","    for file in os.listdir(PATH_TO_MODELS):\n","        if file.startswith(filename):\n","            if file.startswith(f'{filename}_checkpoint{index}'):\n","                pass\n","            else:\n","                os.remove(f'{PATH_TO_MODELS}/{file}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F5R7ySD9xJdW"},"source":["### Picking the best model among all the folds\n","\n","Using **`pick_best_model()`**, we can pick the model among various folds that gives us the best validation accuracy. The best model's name is appended with **_best **in the **models** directory. \n","\n","**Usage:**\n","\n","To pick the best model for **`age`**,\n","```\n","pick_best_model('age')\n","```\n","\n","\n","To pick the best model for **`gender`**,\n","\n","```\n","pick_best_model('gender')\n","```"]},{"cell_type":"code","metadata":{"id":"sKRrMiSI6KsJ","executionInfo":{"status":"aborted","timestamp":1624729800696,"user_tz":-360,"elapsed":22,"user":{"displayName":"Khondoker Ittehadul Islam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GggB1hRCScVHSb8B6_geMRo9IveQWMtqATJkX1z=s64","userId":"10124878052441446029"}}},"source":["def pick_best_model(c):\n","    \"\"\"\n","    Args:\n","        s: A string. Equals either \"train\", \"val\", or \"test\".\n","        c: A string. Equals either \"age\" or \"gender\".\n","    \"\"\"\n","    def fn_filter(file):\n","        file_split = file.split('_')\n","        return True if (len(file_split) == 5 and file_split[1] == c) else False\n","    \n","    def fn_map(file):\n","        file_split = file.split('_')\n","        return get_validation_error(c, file_split[0], file_split[3])\n","    \n","    files = list(filter(fn_filter, os.listdir(PATH_TO_MODELS)))\n","    val_errors = list(map(fn_map, files))\n","    min_val_error, file = min(zip(val_errors, files))\n","    best_model = f'{PATH_TO_MODELS}/{file.split(\".\")[0]}_best.pt'\n","    copyfile(f'{PATH_TO_MODELS}/{file}', best_model)\n","    \n","    print(f'Picking {best_model} as the best model for {c}...')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VhEJNE-PyNqE"},"source":["### Calculating performance/accuracy\n","\n","We can check the performance of any model using the **`get_performance()`** function.\n","\n","**Usage:**\n","\n","To know the performance on **`age`** classification,\n","```\n","get_performance('age')\n","```\n","\n","\n","To know the performance on **`gender`** classification,\n","\n","```\n","get_performance('gender')\n","```\n","\n"]},{"cell_type":"code","metadata":{"id":"OsrQjMrtESM8","executionInfo":{"status":"aborted","timestamp":1624729800697,"user_tz":-360,"elapsed":23,"user":{"displayName":"Khondoker Ittehadul Islam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GggB1hRCScVHSb8B6_geMRo9IveQWMtqATJkX1z=s64","userId":"10124878052441446029"}}},"source":["def get_performance(c):\n","    \"\"\"\n","    Args:\n","        c: A string. Equals either \"age\" or \"gender\".\n","    Returns:\n","        A dictionary containing accuracy (and one-off accuracy for age) of the model.\n","    \"\"\"    \n","    file = get_best_model_filename(c).split('_')\n","    return get_performance_of_a_model('test', file[1], file[0], file[3])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ghOw4Hnpa2wg","executionInfo":{"status":"aborted","timestamp":1624729800700,"user_tz":-360,"elapsed":26,"user":{"displayName":"Khondoker Ittehadul Islam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GggB1hRCScVHSb8B6_geMRo9IveQWMtqATJkX1z=s64","userId":"10124878052441446029"}}},"source":["def get_best_model_filename(c):\n","    def fn_filter(file):\n","        file_split = file.split('_')\n","        return True if (len(file_split) == 6 and file_split[1] == c) else False\n","    \n","    return list(filter(fn_filter, os.listdir(PATH_TO_MODELS)))[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qo_p5XeGUVmd","executionInfo":{"status":"aborted","timestamp":1624729800701,"user_tz":-360,"elapsed":27,"user":{"displayName":"Khondoker Ittehadul Islam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GggB1hRCScVHSb8B6_geMRo9IveQWMtqATJkX1z=s64","userId":"10124878052441446029"}}},"source":["def get_performance_of_a_model(s, c, fold, train_transform_index):\n","    \"\"\"\n","    Args:\n","        s: A string. Equals either \"train\", \"val\", or \"test\".\n","        c: A string. Equals either \"age\" or \"gender\".\n","        fold: An integer. Lies in the range [0, 4] as there are five folds present.\n","        transform_index: An integer. The transforms in the list correesponding\n","            to this index in the dictionary will be applied on the images.\n","    Returns:\n","        A dictionary containing accuracy (and one-off accuracy for age) of the model.\n","    \"\"\"\n","    filename = get_model_filename(c, fold, train_transform_index)\n","    net = Net().to(device)\n","    net.load_state_dict(torch.load(f'{PATH_TO_MODELS}/{filename}'))\n","    performance = test(\n","        net,\n","        get_dataloader(s, c, fold, 0, minibatch_size),\n","        c\n","    )\n","    if c == 'age':\n","        return {\n","            'accuracy': performance['exact_match']/performance['total'],\n","            'one-off accuracy': performance['one_off_match']/performance['total']\n","        }\n","    elif c == 'gender':\n","        return {\n","            'accuracy': performance['exact_match']/performance['total']\n","        }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KQgMcSVibjYa","executionInfo":{"status":"aborted","timestamp":1624729800702,"user_tz":-360,"elapsed":25,"user":{"displayName":"Khondoker Ittehadul Islam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GggB1hRCScVHSb8B6_geMRo9IveQWMtqATJkX1z=s64","userId":"10124878052441446029"}}},"source":["def get_model_filename(c, fold, train_transform_index):\n","    start_of_filename = f'{fold}_{c}_train_{train_transform_index}_checkpoint'\n","    for file in os.listdir(PATH_TO_MODELS):\n","        if file.startswith(start_of_filename):\n","            return file"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aj0XrK-f75EG"},"source":["### How to run?\n","\n","1.   **Run `train_save()`:**\n","\n","\n","> You can do this for different combinations of **`c`**, **`fold`**, and **`train_transform_index`**, where **`c={'age','gender'}`**, **`fold={0,1,2,3,4}`**, and **`train_transform_index={0,1,2}`**.\n","\n","> I suggest you to first train the network on all the folds for **either** **`age`** or **`gender`** and then proceed to the setp 2. Then, follow the same steps for the other class. Also, use **`train_transform_index=2`** as it gives smaller validation error due to random flipping and cropping.\n","\n","> **Note:** It is just the network's architecture that is the same for age and gender. They both are trained independently. Ultimately, we will be having two different networks with the same architecture, one to classify age and the other to classify gender.\n","\n","2. **Picking the best model:**\n","\n","> Call **`pick_best_model()`** on either **`age`** or **`gender`**.\n","\n","3. **Know the performance:**\n","\n","> Call **`get_performance()`** on either **`age`** or **`gender`** to know the final performance of the network on the test set.\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"PqPEPILSzrKW"},"source":["# Results"]},{"cell_type":"markdown","metadata":{"id":"Qi1mAy6-CeOK"},"source":["## Age"]},{"cell_type":"markdown","metadata":{"id":"oQ483io4Ctax"},"source":["### Accuracy\n","These are the accurcies we get on calling **`get_performance()`** on **`age`**.\n","\n","```\n","{'accuracy': 0.5272136474411048, 'one-off accuracy': 0.8399675060926076}\n","```\n","\n"]},{"cell_type":"markdown","metadata":{"id":"5JNvwScSwz8u"},"source":["## Gender"]},{"cell_type":"markdown","metadata":{"id":"WzBBnRYpNqsH"},"source":["### Accuracy\n","This is the accuracy we get on calling **`get_performance()`** on **`gender`**.\n","\n","```\n","{'accuracy': 0.8437770719029744}\n","```\n","\n"]},{"cell_type":"markdown","metadata":{"id":"fmgl8BZyZToa"},"source":["# Testing on an image"]},{"cell_type":"markdown","metadata":{"id":"5nLgK_DMQVpL"},"source":["You can either download the trained models for age and gender classification from [here](https://github.com/droidadroit/age-and-gender-classification/tree/master/models) into the **models** directory or rename the best models for age and gender upon training with **age.pt** and **gender.pt** respectively.\n","\n","\n","For the prediction of age and gender from an image, simply call **`test()`** providing the path (in your Google Drive) of the image as the argument.\n","\n","**Usage:**  \n","`test(\"/content/gdrive/My Drive/AgeGenderClassification/test.jpeg\")`"]},{"cell_type":"code","metadata":{"id":"aeIlSTv2KAQt","executionInfo":{"status":"aborted","timestamp":1624729800703,"user_tz":-360,"elapsed":26,"user":{"displayName":"Khondoker Ittehadul Islam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GggB1hRCScVHSb8B6_geMRo9IveQWMtqATJkX1z=s64","userId":"10124878052441446029"}}},"source":["PATH_TO_AGE_MODEL = f'{PATH_TO_MODELS}/age.pt'\n","PATH_TO_GENDER_MODEL = f'{PATH_TO_MODELS}/gender.pt'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m8UQqvFby3XK","executionInfo":{"status":"aborted","timestamp":1624729800704,"user_tz":-360,"elapsed":27,"user":{"displayName":"Khondoker Ittehadul Islam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GggB1hRCScVHSb8B6_geMRo9IveQWMtqATJkX1z=s64","userId":"10124878052441446029"}}},"source":["mapping = {\n","    0: '0-2 years',\n","    1: '4-6 years',\n","    2: '8-13 years',\n","    3: '15-20 years',\n","    4: '25-32 years',\n","    5: '38-43 years',\n","    6: '48-53 years',\n","    7: '60 years and above',\n","    8: 'male',\n","    9: 'female'\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VG9zRkATZW1C","executionInfo":{"status":"aborted","timestamp":1624729800705,"user_tz":-360,"elapsed":28,"user":{"displayName":"Khondoker Ittehadul Islam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GggB1hRCScVHSb8B6_geMRo9IveQWMtqATJkX1z=s64","userId":"10124878052441446029"}}},"source":["def test_on_a_class(c, image_tensor):\n","    with torch.no_grad():\n","        net = Net().to(device)\n","        net.load_state_dict(torch.load(f'{PATH_TO_MODELS}/{c}.pt'))\n","        net.eval()       \n","        output = net(image_tensor)\n","        output = torch.max(output, 1)[1].to(device)\n","        result = f'{c} = {mapping[output.item()]}'\n","\n","    return result    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vlrupi7_EXYY","executionInfo":{"status":"aborted","timestamp":1624729800706,"user_tz":-360,"elapsed":29,"user":{"displayName":"Khondoker Ittehadul Islam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GggB1hRCScVHSb8B6_geMRo9IveQWMtqATJkX1z=s64","userId":"10124878052441446029"}}},"source":["def test(path):\n","    image = Image.open(path)\n","    plt.imshow(image)\n","    image = transforms.Compose(transforms_dict['test'][0])(image)\n","    image.unsqueeze_(0)\n","    image = image.to(device)\n","    print(test_on_a_class('age', image))\n","    print(test_on_a_class('gender', image))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EpNbkMh_RNKg"},"source":["## Examples"]},{"cell_type":"code","metadata":{"id":"wAvhu1z6RIRX","executionInfo":{"status":"aborted","timestamp":1624729800707,"user_tz":-360,"elapsed":30,"user":{"displayName":"Khondoker Ittehadul Islam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GggB1hRCScVHSb8B6_geMRo9IveQWMtqATJkX1z=s64","userId":"10124878052441446029"}}},"source":["test(\"/content/gdrive/My Drive/AgeGenderClassification/images.jpg\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-EqG8ushRHfv","executionInfo":{"status":"aborted","timestamp":1624729800708,"user_tz":-360,"elapsed":31,"user":{"displayName":"Khondoker Ittehadul Islam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GggB1hRCScVHSb8B6_geMRo9IveQWMtqATJkX1z=s64","userId":"10124878052441446029"}}},"source":["test(\"/content/gdrive/My Drive/AgeGenderClassification/download.jpg\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ki0hQatZBRTA"},"source":["# Issues while running:\n","\n","\n","* \n","**```\n","AttributeError: module 'PIL.Image' has no attribute 'register_extensions'\n","```**\n","\n","\n","> In such cases, restart the runtime by pressing **Ctrl + M** or going to **Runtime->Restart runtime** in the menu bar of Colab. Then, run the code again.\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"G33VhSaDnvaC","executionInfo":{"status":"aborted","timestamp":1624729800710,"user_tz":-360,"elapsed":33,"user":{"displayName":"Khondoker Ittehadul Islam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GggB1hRCScVHSb8B6_geMRo9IveQWMtqATJkX1z=s64","userId":"10124878052441446029"}}},"source":[""],"execution_count":null,"outputs":[]}]}